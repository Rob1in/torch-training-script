# Configuration for hyperparameter optimization with Optuna
# Usage: python src/train.py --config-name=sweep --multirun
#
# Requirements: pip install -e ".[sweep]"

# Hydra Optuna Sweeper Configuration
hydra:
  sweeper:
    _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper
    sampler:
      _target_: optuna.samplers.TPESampler
      seed: 42
    direction: minimize
    study_name: object_detection_sweep
    storage: null  # Use in-memory storage (or specify sqlite:///optuna.db for persistence)
    n_trials: 30
    n_jobs: 1
    params:
      # Model architecture (optional: sweep over models)
      # model: choice(faster_rcnn, ssdlite, effnet)
      
      # Hyperparameters to optimize (PyTorch reference style)
      training.learning_rate: tag(log, interval(1e-4, 1e-2))   # Search around 0.0025 (PyTorch default for 1 GPU)
      training.weight_decay: tag(log, interval(1e-6, 1e-3))    # Around 1e-4 base
      training.momentum: tag(uniform, interval(0.85, 0.95))    # Around 0.9 base

defaults:
  - model: faster_rcnn  # Choose: faster_rcnn, ssdlite, effnet
  - dataset: jsonl
  - override hydra/sweeper: optuna
  - _self_

# Experiment settings
experiment:
  name: sweep_${now:%Y-%m-%d}_${now:%H-%M-%S}
  seed: 42
  device: cuda

# Classes configuration
# List of annotation labels to train/evaluate on.
# Only annotations with labels in this list will be included.
# If None or empty, all annotation labels found in the JSONL file will be used.
classes:
  - triangle
  
# Training configuration (starting values, will be overridden by Optuna)
training:
  # Batch size - use 16 for 24GB GPU
  batch_size: 16
  num_epochs: 15  # Shorter for sweeps
  
  # Optimizer settings (PyTorch reference)
  optimizer: sgd  # sgd or adam
  learning_rate: 0.0025  # Will be overridden by Optuna (PyTorch default for 1 GPU)
  momentum: 0.9  # Will be overridden by Optuna
  weight_decay: 0.0001  # Will be overridden by Optuna
  norm_weight_decay: null  # Weight decay for normalization layers (null = same as weight_decay)
  nesterov: false
  
  # Learning rate schedule (PyTorch reference)
  lr_steps: [8, 11]  # Reduce LR at these epochs (adjusted for shorter training)
  lr_gamma: 0.1  # Multiply LR by this factor
  
  # Warmup settings (applied only in epoch 0)
  warmup_iters: 1000  # Number of iterations for warmup in first epoch
  warmup_factor: 0.001  # Start at 0.1% of base LR (1/1000)
  
  # Training settings
  gradient_clip: 0.0  # Max gradient norm (0 to disable, PyTorch reference doesn't use it)
  early_stopping_patience: 10  # Shorter patience for sweeps
  checkpoint_dir: checkpoints
  num_workers: 1
  pin_memory: false
  
  # Model EMA (Exponential Moving Average)
  use_ema: true  # Enable EMA for more stable models (+0.5-1.0 mAP)
  ema_decay: 0.9998  # EMA decay rate (0.9998 or 0.9999)
  
  # Transfer Learning: Layer Freezing
  # NOTE: freeze_fpn and freeze_rpn only apply to Faster R-CNN
  freeze_backbone: false  # ALL MODELS
  freeze_fpn: false       # FASTER R-CNN ONLY
  freeze_rpn: false       # FASTER R-CNN ONLY
  freeze_all: false       # ALL MODELS

# Logging
logging:
  save_dir: ${hydra:runtime.output_dir}

evaluation:
  confidence_threshold: 0.7
